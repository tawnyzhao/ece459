# Lecture 22 â€” GPU Programming Continued

## Roadmap

Having talked about the GPU side in Lecture 21, let's continue and talk about
the host (CPU) side here.

## Intro

### Execution

[Heterogeneous
Programming](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#heterogeneous-programming)

Advanced: can we call `launch` inside kernel on device? (Yes, see
[here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-environment-and-memory-model))

### Thread Hierarchy

* If a thread has index `(x, y)` in a block of size `(Dx, Dy)`, what is the
thread ID of it? If a thread has index `(x, y, z)` in a block of size `(Dx, Dy,
Dz)`, what is the thread ID of it? (The thread ID is the same as its index for a
one-dimensional block)

* Are thread blocks always being executed in parallel?

* Advanced: how are the blocks executed?

Answers:

* `x + y Dx` and `x + y Dx + z Dx Dy` (see
  [here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy))

* No, they can be executed in any order (see
  [here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#scalable-programming-model-automatic-scalability))

* Advanced: Each block is executed in groups of 32 threads by multiprocessors
  called warps. The number of wraps of a block is:
  $\text{ceil}\left( \frac{T}{W_{size}},1 \right)$
  (see [here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation))

### Memory Usage

Let's only focus on the following two.

* shared memory: only within thread block, but fast
* global memory: persistent across kernel launches by the same application, but
slow

Exercise: given above, how can you speed up matrix multiplication? (See the
answer
[here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory))

## Live coding [20 minutes]

* The sample host code in the lecture notes. See
<https://github.com/bheisler/RustaCUDA/blob/6b076722d593288a1fe58e8d7626e51da158e2f1/examples/launch.rs>

* Also it is worth reading rustacuda's [official
  doc](https://docs.rs/rustacuda/latest/rustacuda/)

* check the runtime
  * nbody sequential, no bins (`live-coding/L13/nbody`),
  * nbody-parallel, no bins (`live-coding/L13/nbody-parallel`)
  * nbody-cuda (`live-coding/L23/nbody-cuda`)

Mention `nvprof` but we won't use it yet (L27), but we can try [time
functions](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html?highlight=shared#time-function)


## Experimentation [45 minutes]

Again with the `calculate_forces` code we (you) can experiment with tweaking the
threads per block and the grid size, and see how fast it is with various
choices. There's also an issue with dividing evenly---check your accelerations
to make sure that you're not getting 0s where you shouldn't be---and with
indexing with respect to the work-item ID. The version you can tweak is in
`live-coding/L22/nbody-cuda`. You need to tweak the launch call and the
`calculate_forces` call.

If you just want to look at the answer (less educational!), it's in
`live-coding/L22/nbody-cuda-grid`.

# After-action report, huanyi, 15Feb23

Used the documents in the lab 3 manual as well, which I think was useful.

# After-action report, huanyi, 04Mar24

I added more notes to better explain some of the terminologies and I covered
them all. Therefore, I did not have time to do the live-coding, but I did
explain the host-side code, although RustaCUDA seems to be not maintained
anymore. I did not have students do the experiments, either, which was left as
an after-class practice.
