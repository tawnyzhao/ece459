# Lecture 21 â€” GPU Programming (CUDA)

## Roadmap

This lecture is about understanding how to write GPU kernels. We have an
exercise which will help you get a better feel for

* warmup time;
* an active data parallelism exercise; and
* live coding about kernels

## Warm-up overheads [5 minutes]

This exercise works on your own or with a partner.

1. How long does it take to get from Parliament Hill in Ottawa to Gare Centrale
in Montreal? Estimate the time by car and by plane. Assume that you are driving
from Parliament Hill to the Ottawa airport and also from Montreal Trudeau
airport to Gare centrale. Don't forget that you have to clear security and be
there on time to board.

2. Same question, but from the Davis Centre at the University of Waterloo to the
Ferry Building in San Francisco. To keep things simple, let's say that you are
taking a taxi or rideshare to and from the airport.

## Data parallelism (10 minutes)

* The idea is that we evaluate a function, or
  [kernel](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#kernels),
  at a set of points. The set of points is also named *index space*. Each of the
  points corresponds to a *work item*.

* The unit of execution in a GPU is called a *warp*, which executes SIMT
  ([Single-Instruction,
  Multiple-Thread](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture))
  instructions. There can be multiple units of execution in a given GPU.

* Each data element is a work item. Commonly, you want CUDA to spawn a thread
  for each work item, with a unique thread ID; they are grouped into blocks (see
  [Grid of Thread
  Blocks](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy-grid-of-thread-blocks)).
  Each block has to be able to execute independently

* Discuss the memory types (A useful material may be
  [link](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy))
  * per-thread registers and local memory
  * shared memory (shared within blocks)
  * distributed shared memory (Only Thread Block Cluster is supported)
  * global memory (shared between all GPU kernels)
  * two more but we may or may not care that much (see [memory
    model](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html?#memory-model))
    * constant memory (read-only, optimized for immutable variables)
    * texture memory (mostly read-only, optimized for 2D spatial access)

* Branches. In practice, the hardware will execute all branches that any thread
  in a warp executed (due to SIMT).
  [This](https://stackoverflow.com/questions/17223640/is-branch-divergence-really-so-bad)
  may help.

* Atomic functions. For example, two work items somehow touch the same memory
  location.

## Live coding: kernels (10 minutes)

* show the simple sum kernel and run it (see `live-coding/L21/`)

## Activity

TODO: find a better example

Project the N-body problem code from L22. Each student is a work-item. Assign a
grid number to each student on the 2D grid induced by the classroom layout. So,
of course, each student runs a thread. You can pick your inputs arbitrarily,
just do the calculation. Group into blocks. Blocks share memory. Only some
number of students can run simultaneously (limited by GPU characteristics).

## Guided discussion: writing a kernel well (10 minutes)

Run through the discussion in the L22 notes under "Writing a kernel well" and
collectively develop the points there.

# After-action report, huanyi, 01Mar24

I did the warm up, talked about the and talked about the concepts, and tried the
simple sum kernel. I talked about "writing a kernel well" but I think there
could be some coding exercises.
